%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments at Dropbox}
\label{sec:dropbox}

Our Mechanical Turk experiments in the last section show that there exists a
small set of frequently observed typos. Those experiments, which asked users to type
in passwords provided to them, may not simulate the kinds of typos users make
when using their own passwords. We therefore turn to investigating typos in the
production password authentication environment used at Dropbox. We will also
assess the impact of typos on user experience. We emphasize that our experiments
here did not change the effective login checks at Dropbox, but only recorded
information about the frequency of typos. 

\paragraph{The Dropbox authentication system.} Dropbox is a file
hosting service for consumers and enterprises with hundreds of
millions of users. Each user must select a password during
registration. Dropbox uses zxcvbn~\cite{dropbox2012zxcvbn}, a password
strength estimator, to guide the user in choosing a strong
password. The system requires that users choose a password of at least
six characters, but it does not explicitly forbid users from choosing
passwords that are considered to be weak by zxcvbn. Passwords are
submitted over a standard HTTPS POST interface when logging in via the
website or from within one of the native Dropbox applications. We call the
submission of a password by a user a {\em password submission}.  If the
password is accepted by the Dropbox server, we call it a  successful
password submission, otherwise it is called a failed password
submission.  On a failed password submission, the user may resubmit his/her
password. A {\em login attempt} is a sequence of password submissions by a user that either
culminates in a successful login, in which case the login attempt is considered successful, or accumulates login failures until the study ends. If
the user does not
succeed in logging in during the scope of our study, we consider her sequence of password submissions to be a failed login attempt.


Dropbox, like most modern web companies, uses a number of fraud detection
mechanisms in order to filter out spurious login attempts even before checking
the password. An example of such fraud detection mechanisms is to refuse login
attempts from IP addresses that appear on a blacklist for known bots. While
some spurious login submissions may make it through these filtering mechanisms,
we assume for simplicity below that our instrumentation is only monitoring
legitimate login attempts. Note that this is a conservative assumption: if the
data we collected contains illegitimate login attempts, then the true rate of
correctable typos for legitimate users would be even higher. Our security analyses
(\secref{sec:security}) will not make such assumptions.
%For the purposes of our study below, we assume any password login attempt We
%assume this does a good job at filtering 


\iffalse
lowercase first character, uppercase first character, remove first character, remove last character, swap case of entire password
\fi


%\tnote{Below I am using typo to refer to corrections. We need to make a pass to
%render homogenous the terminology and notation.}


\paragraph{Instrumentation.} We modified the Dropbox password checking code to
perform additional checks on all legitimate login attempts on the web
interface. This provided a vast amount of data, and it eliminated biases that
could arise from selecting some small percentage of accounts. This also made visible multiple password submissions from a single user, which was necessary for timing
re-tries.

During the period of measurement, every password submission was
processed as follows. If the password check passed, do nothing.
Otherwise, if it failed, apply one or more typo corrections from some
predefined corrector function set $\typoset = \{f_1,f_2,\ldots,f_c\}$
where corrector functions were defined in the last section. We used
slightly different sets of correctors in different experiments, as
discussed below.  One or more of the corrected version(s) of the
password are checked. For failed login attempts, a log entry was
generated that contained a time stamp, whether login would have been
successful with a correction of the password, the type of correction $f_i$
that was successful (if applicable), and the user agent string.

We emphasize that in our experiments login is \emph{not} allowed based on the
corrected passwords. We did not modify Dropbox's effective login checks; we
only collected the data needed to evaluate whether doing so would be
beneficial.


\paragraph{Typos and login failure rates.} In an initial experiment we
set out to measure the incidence rate of the top five corrections seen
in the MTurk study of \secref{sec:mturk}. Thus for this
experiment the set of corrector functions is
$\topfive = \{\swcall,\swcfirst,\rmlast,\rmfirst,\dtoslast\}$.  For
each instrumented failed password submission, one correction from
$\topfive$ was chosen uniformly at random and applied to the submitted
password.  The reason is that, in the current implementation, only
sequential code is easily supported, and the password hashing scheme
used at Dropbox is (by design) slow to compute. It was unclear a
priori exactly what overhead the additional checks would have on
Dropbox infrastructure, and so we conservatively only performed one
additional check at a time.
%ensured that latency of failed login
%attempts was only modestly increased, while still allowing us to collect
%aggregate statistics on the prevalence of each of the typos checked for overall.
The success of this initial experiment suggested the performance impact was low,
and later experiments applied multiple corrections (see below).
We collected information over a 24-hour period.

We cannot report on the exact number of login attempts during this
period, as this is considered confidential information by Dropbox. We will
therefore report only rates of success and failure.
% Dropbox
%has upwards of 
%\item %more than \fixme{500 million} customers, with one percent of users therefore
%being in the order of millions of customers. 
%The number of instrumented login attempts over the 24 hour period was
%\fixme{XXX}.  
%To make this clear, we need to fix some notation. 
In the following, we let $c_f$ denote the number of times a corrector $f$ was
applied to an incorrect password during an experiment. We let $r_f$  be the
number of times $f$ successfully corrected an incorrect password during the
experiment. The ratio $r_f / c_f$ gives the percentage 
of login failures correctable by $f$.


%Let $n$ be the total number of password submissions to the Dropbox
%authentication server in the 24-hour period, $a$ be the number of
%accepted password submissions, and $r$ be the number of rejected
%password submissions. For each incorrect password submission, one of
%the correctors is selected at random and used to correct the submitted
%password. Let $c_f$ be the number of times a corrector $f\in\topfive$
%is selected and $r_f$ be the number of time the correction was
%successful. Then, $ n = a + r = a + \sum_{f\in\topfive} c_f \;.$
%The ratio $r_f / c_f$ gives an estimate of the percentage of typos correctable
%by $f$. 
% Number of rejected submissions that could be corrected by
% $f \in \topfive$ is denoted by $r_f$, all other rejected submissions
% that cannot be corrected by any $f\in\topfive$ are denoted by
% $r_\textnormal{other}$.  Thus \bnm n = a + r = a + r_\textnormal{other} +
% \sum_{f\in\typoset} r_f \;.  \enm Now, let $r'_f$ be the measured
% number of password submissions that were correctable by $f \in \typoset$. Because
% we choose one corrector at random from $\topfive$ for each failed
% password submission, this measured number is not equal to the true
% number.  We can however estimate $r_f$ by $\hat{r}_f = |\topfive| \cdot r'_f = 5\cdot r'_f$.
% As mentioned, confidentiality concerns prevent us from reporting absolute
% numbers.

%, we report in the middle column
%the measured number of password submissions corrected by the corrector
%$f$ as a fraction of the total number of failed login attempts. In the
%far right column we give $r_f/c_f$, that is the estimated fraction of
%typos that can be corrected by the corrector $f$. The fraction of time
%a corrector is chosen is very close to 20\% for each corrector, hence
%we omit that column from the table.

% \footnote{Multiplication was performed before rounding, 
% so the latter column is not always exactly a multiple of five of the first
% column.}
% $\hat{r}_f / r = 5 \cdot r'_f/r$.

\pgfplotstableread[col sep=comma, format=inline] {
index,type,mobile,desktop
0,\swcall,0.383,1.21
1,\swcfirst,5.88,5.51
2,\rmlast,1.87,2.06
3,\rmfirst,0.390,0.347
4,\dtoslast,1.82,0.0367
}\devicedist

\begin{figure*}[t]
% \label{fig:dropbox-summary-one}
\begin{center}
\hpagesss{0.3}{0.3}{0.3}{
\vspace{0.3in}
\begin{tabular}{lr}
  \toprule
  Corrected by ($f$) & { $r_f/c_f$ (\%)}\Tstrut  \\
  \midrule
  $\swcall$   & 1.13\\ % & 1.13 \\
  $\swcfirst$ & 5.56\\ % & 5.56 \\
  $\rmlast$   & 2.05\\% & 2.05 \\
  $\rmfirst$  & 0.35\\ % & 0.35 \\
  $\dtoslast$ & 0.21\\ % & 0.21 \\
  \midrule
  $\topfive$   & 9.30\\ % & 9.30 \\
  \bottomrule
\end{tabular}}{
\vspace{0pt}
\begin{tikzpicture}[scale=0.5]
  \begin{axis}[
    axis y line=left,
    axis x line=bottom,
    ymin=0, ymax=6,
    xlabel={\Large Corrector $f$},
    ylabel={\Large \% of failures corrected ($r_f/c_f$)},
   %    y axis style=black!75!black,
    xtick=data,        
    xticklabels from table={\devicedist}{type},
    % xtick={{\large \swcall}, {\large \swcfirst}, {\large \rmlast}, {\large \rmfirst}, {\large \dtoslast}}
    legend style={at={(0.97,0.97)}, anchor=north east},
    ybar,
    enlarge x limits=0.3,
    % cycle list={
    % {fill=black!50,draw=black!50},
    % {fill=black!80,draw=black!80},
    % }
    ]
    \addplot [  black,
    fill=blue!40,
    postaction={
      pattern=north east lines
    }] table [x={index}, y={desktop}]  \devicedist;
    \addplot table [x={index}, y={mobile}]  \devicedist;
    \legend{Desktop, Mobile}
  \end{axis}
\end{tikzpicture}}{
% \label{fig:delay-cdf}
\vspace{0pt}
\begin{tikzpicture}[scale=0.5]
  \begin{axis}[
      axis x line=bottom,
      axis y line=left,
      xmode=log,
      xlabel={\large Time delay (s)},
      ylabel={\large \pbox{2in}{Fraction of users \\w/ multiple login attempts}}
      ]
      \addplot [blue, mark=none] table [col sep=comma,
      x=fraction of users, y=seconds saved] {images/cdf.csv};
    \end{axis}
  \end{tikzpicture}}
\end{center}
%\begin{tabular}{lrr}
%\toprule
%   Corrected by ($f$)& { $r_f/r$ (\%)}\Tstrut & { $r_f/c_f$ (\%)} \\
%\midrule
%$\swcall$   & 0.226  & 1.13\\ % & 1.13 \\
% $\swcfirst$ & 1.112  & 5.56\\ % & 5.56 \\
% $\rmlast$   & 0.407  & 2.05\\% & 2.05 \\
% $\rmfirst$  & 0.070  & 0.35\\ % & 0.35 \\
% $\dtoslast$ & 0.043  & 0.21\\ % & 0.21 \\
%\midrule
% $\topfive$  & 1.858  & 9.30\\ % & 9.30 \\
%\bottomrule
%\end{tabular}
%}
  \caption{\textbf{(Left)} The fraction of failed logins correctable
    by $\topfive$ in a 24-hour study at Dropbox. \textbf{(Middle)}
    Performance of $\topfive$ on mobile versus desktop. For each
    corrector in $\topfive$ we plot the fraction of failures for each
    platform correctable by the corrector. \textbf{(Right)} CDF of
    time delay (in seconds) between the first failed login due to a
    typo and first successful login. Included are only users that had
    a failed login attempt and later a successful one.
%$\topfive$.
  %in $\topfive$, chosen uniformly, would
  %have enabled login. 
  %Here $r$ is the number of failed login attempts,
  %$c_f$ is the number of times the corrector $f$ is selected to
  %correct the submitted password, and $r_f$ is the number times the
  %correction was successful. 
  %The observed and estimated fraction of
  %failed login attempts that can be corrected are
  %tabulated in the middle and the right column respectively.
  % The middle column is
  % the fraction of failed login attempts observed to be correctable the
  % corrector, and the far right column reports the estimated fraction
  % of failed logins that are correctable by the corrector. 
}
\label{fig:dropbox-summary-one}
\end{figure*}

%We give the breakdown of the measured rate of corrector function   
%A summary of the main statistics regarding failure rates and typo
%rates are given in \figref{fig:dropbox-summary-one}.


%$\cup \{\otherfail\}$ where $\otherfail$ is a catch-all for failed logins that 
%weren't detected as correctable (either they were not a typo correctable by a
%mechanism in $\typoset$ or was a typo correctable by one of the corrections in
%$\typoset$ that was not selected for that login attempt).  
%Thus $\hat{n} = 1 = \hat{s} + \hat{f}_\otherfail +
%\sum_{T\in\typoset} \hat{f}_T$.  Then because we sample randomly from
%$\typoset$, we expect that 

%rate of correctable failures of any type in $\typoset$ can
%be estimated by $|\typoset|\cdot\sum_{T\in\typoset}\hat{f}_T/\hat{n}$.  The rate
%of typo failures of a certain type $T\in\typoset$ is estimated by
%$\hat{f}_T/\hat{n}$. 


The left figure of \figref{fig:dropbox-summary-one} reports
the measured ratios $r_f/c_f$ for each corrector in $\topfive$ in
during the 24-hour period. 
This reveals that 9.3\% of failures are due to typos correctable by
$\topfive$, suggesting that typos indeed account for a significant
number of failed (legitimate) password submissions\footnote{We can add
  the fractions of typos because our correctors are mutually
  exclusive.}. By correction type, we see that the
most common correction (switching the case of the first character)
accounts for 60\% of these, and the first three (switching the case of
all characters, just the first character, dropping the last character)
account for over 90\% of these. Apparently capitalization errors are a
significant source of errors, which provides evidence for why Facebook
accepts these typos.

Some disparity with the MTurk results is apparent. While the top three of
these five correctors are the same, the ordering is distinct, with caps-lock errors
proportionally higher in MTurk then here. We believe this is due to the MTurk
experiment design, and that the Dropbox numbers more accurately reflect rates in operational environments.

While collecting this data, we recorded the user agent for all
password submissions, so we were able to analyze the performance of
typo correction on mobile platforms versus desktop platforms.  We
found that the estimated correction rate for mobile was slightly
higher at 10.5\%, compared to 9.3\% for desktop (calculated here with
the denominator being the number of rejected password submissions for
mobile and desktop, respectively). We show, in the middle figure of
\figref{fig:dropbox-summary-one}, the estimated correction rates for
each user agent broken down by corrector function.  We see that
\dtoslast is a significantly more effective correction on mobile,
which may be because mobile keyboards require switching to an
alternate keyboard to reveal symbols.  We also see that \swcall is a
more effective correction on desktop, most likely because it's easier
to leave caps lock enabled on conventional keyboards.\footnote{On
  Android devices, enabling caps lock requires pressing and holding
  the shift button, and on iPhone devices one has to double press the
  shift button to enable caps lock.}  %% Looking
%% ahead to the next section,
This dichotomy suggests the potential merit
of applying different correction policies on the server based on the
user agent. We leave the further analysis of this for future work. 


% \pgfplotstableread[col sep=comma, format=inline] {
% index,type,mobile,desktop
% 0,\swcall,0.383,1.21
% 1,\swcfirst,5.88,5.51
% 2,\rmlast,1.87,2.06
% 3,\rmfirst,0.390,0.347
% 4,\dtoslast,1.82,0.0367
% }\devicedist

% \begin{figure}[t]
%   \centering
%     \begin{tikzpicture}[scale=0.5]
%       \begin{axis}[
%         axis y line=left,
%         axis x line=bottom,
%         ymin=0, ymax=6,
%         xlabel={\Large Corrector $f$},
%         ylabel={\Large \% of failures corrected ($r_f/c_f$)},
% %        y axis style=black!75!black,
%         xtick=data,        
%         xticklabels from table={\devicedist}{type},
%         %xtick={{\large \swcall}, {\large \swcfirst}, {\large \rmlast}, {\large \rmfirst}, {\large \dtoslast}}
%         legend style={at={(0.97,0.97)}, anchor=north east},
%         ybar,
%         enlarge x limits=0.25,
%         %cycle list={
%         %  {fill=black!50,draw=black!50},
%         %  {fill=black!80,draw=black!80},
%         %}
%         ]
%         \addplot [  black,
%         fill=blue!40,
%         postaction={
%           pattern=north east lines
%         }] table [x={index}, y={desktop}]  \devicedist;
%         \addplot table [x={index}, y={mobile}]  \devicedist;
%         \legend{Desktop, Mobile}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Performance of $\topfive$ on mobile versus desktop. For
%       each corrector in $\topfive$ we plot the fraction of
%       failures for each platform correctable by the corrector.}
%   \label{fig:device-dist}
% \end{figure}



%Of the \fixme{12\%} of failed login attempts, we estimate that 
%to 
%We saw a significant number of failed login attempts at \fixme{20\%} of all
%login submissions. Still measuring relative to all login attempts,
%\fixme{$\hat{f}_T = 10\%$} were absolutely seen caught as one of the typos seen.
%This is \fixme{50\%} of all failures, suggesting that typos indeed account for a
%significant number of failed legitimate login attempts. 
%, \fixme{20\%} of submitted passwords failed to be correct and

%In \figref{fig:dropbox-summary-one} we provide a breakdown of the observed
%typo rates for those checked in $\topfive$. We see that caps lock is by far the
%largest fraction of typo errors, with first-character capitalization being the
%second most frequently occurring. The top three typos account for \fixme{80\%}
%of observed typos.  
%Note that because we probabilistically check one typo in $\topfive$, some
%numbers must be extrapolated.


\paragraph{Utility of the top three corrections.} We perform a second
study that restricts attention to just the overall top three
correctors $\topthree = \{\swcall,\swcfirst,\rmlast\}$ observed in the
previous study (and, in turn, the MTurk experiments). For this
experiment, the instrumentation applied all three correctors to any
password that failed to exactly match the registered password. So,
now $c_f$ is the number of failed login attempts for every $f \in \topthree$. %and
% so we no longer need to correct by a factor of $|\topthree|$ to perform
% estimates; the measured rate $\hat{r}_f / r$ is all what we need.
As before, we recorded data for 24 hours.

We additionally recorded the time duration for a login attempt to
succeed. That is the time lag between the first failed submission and
the first successful submission by each user in this 24-hour
period. (Because Dropbox uses session cookies most users typically
need to successfully login only once per 24-hour period.) This allowed
us to quantify the time delay between failures and successes, a
measure of how much utility is lost due to usability issues such as
typos.

As we would expect, the success rate of corrections closely matched
the results of the previous 24-hour experiment. Specifically, typos correctable
by $\topthree$ accounted for 9\% of failed password submissions. 
This also attests the stability of these percentages over time.

We show in right figure of \figref{fig:dropbox-summary-one} a CDF of
the delay in logging in over all users who eventually succeeded at
logging in (within the 24-hour period).  Note that some small fraction
of users did not log in for a very long time, suggesting they gave up
and came back hours later. Even so, almost 20\% of users that
experienced a failed login would have been logged in a minute earlier
should typo-tolerant checking have been enabled. Aggregated across all
failed login attempts, typo-tolerance here would have \emph{increased
  logged in time by several person-months just for this 24-hour
  experiment}.
%{\bf \textcolor{\notecolor}{$\ll$Anish: {\sf This ``several person-months'' is
%slightly misleading, because it takes into account those people who tried, say,
%logging in at 1pm (and got close enough with their password), but failed to log
%in, gave up, and came back a couple hours later... that isn't really a couple
%hours of wasted time. Should we get rid of this statement?}$\gg$}}
This represents a significant impact on
user experience and a clear pain point for companies keen on making it easy
for their users to log in.


In aggregate, of all users who attempted to log
into Dropbox within the 24-hour measurement period, we discovered that 3\% were turned
away even though at least one of their submitted passwords was
correctable by one of the correctors in $\topthree$. % Allowing typo
% tolerance with correctors in $\topthree$ would have increased the
% number of successful logged in users by a significant amount. 
This
also represents a significant impact on user experience, with users being
prevented from using the service.

% Every day, some users attempt to log in and use Dropbox, but they give up
% before successfully authenticating. Of the total number of people who visit the
% site in a 24-hour period with the intent of logging in, 3\% of the attempts are
% preventable failures: situations where the user gave up before successfully
% logging in, but where one of their password attempts was close enough
% (correctable by one of the three corrections) to have been logged in 
% were typo correction enabled. This also represents a significant
% impact on user experience, with users prevented from using the product.
% %when they could have been logged in with negligible impact on security.


\iffalse
\bigskip
\bigskip



\begin{newenum}
\item Description of basic experiment.  Explain
statistical sampling
\item Basic stats on rate of failed logins, percentage caught by typo checks
\item Experiment two: determining time saved and such
\end{newenum}

The experiments on MTurk discussed in the previous section provide a potential
model of the distribution of typos seen in practice. As previously noted, those
experiments however have some limitations, in particular the ecological validity
of the experimental MTurk procedure may not hold. Most obviously, that the types
of typos made differ for memorized versus transcribed passwords. 

In this section we report on at-scale experiments performed on Dropbox's
production password checking infrastructure. We perform hypothesis testing to
show that the distributions of typos that we come up using the data collected
from Mechanical Turk experiment is very close. \\

We want test following hypothesis -- 
\begin{itemize}
\item Are the percentage of total mistyped password in Mechanical Turk and that in
  Dropbox server same?
\item Are the most probable typos and their percentages similar across experiments?
\item Can the security analysis using RockYou password leak be generalized for
  other password distribution, especially the one of Dropbox? It will great if
  we can validate this, but probably we cannot.
\end{itemize}


[[What do we want to do to analyze the results of MTurk study?]]
[[Chi-squared to show that things match... use other for tail and we should have
a small categorical support]]
\fi


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
